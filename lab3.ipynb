{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNzw55ouNwWkhsdQD5bcRSB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#LAB-03\n","##Implement the feed forward nueral network with tanh activation function in hidden layer and sigmoid in output layer.\n","##implemenat a program aimed at constructing and training a multilayer perceptron tailored for a specific task showcasing the execution of back propagartion construct a ntw with a linear input layer, tanh and relu activation for the hidden layers and sigmoid and softmax activation for the output layer"],"metadata":{"id":"69aAVtnMrI0f"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wdHUPXoQrFam","executionInfo":{"status":"ok","timestamp":1729591320658,"user_tz":-330,"elapsed":459,"user":{"displayName":"Nithesshkumar Jeer Sridhar","userId":"12749079037239522902"}},"outputId":"b6ce3f12-9f18-4b25-99c4-789aae5e6045"},"outputs":[{"output_type":"stream","name":"stdout","text":["Output probabilities for each input:\n","[[0.5        0.5       ]\n"," [0.78623273 0.63744365]\n"," [0.41956285 0.44694287]\n"," [0.76885266 0.61913015]]\n","\n","Predictions:  [0 0 1 1 0 0 1 1]\n"]}],"source":["import numpy as np\n","\n","def tanh(x):\n","    return np.tanh(x)\n","\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","X = np.array([[0, 0],\n","              [0, 1],\n","              [1, 0],\n","              [1, 1]])\n","y = np.array([[0],\n","              [1],\n","              [1],\n","              [0]])\n","\n","input_layer_size = 2\n","hidden_layer_size = 2\n","output_layer_size = 2\n","np.random.seed(42)\n","weights_input_hidden = np.random.randn(input_layer_size, hidden_layer_size)\n","bias_hidden = np.zeros((1, hidden_layer_size))\n","\n","weights_hidden_output = np.random.randn(hidden_layer_size, output_layer_size)\n","bias_output = np.zeros((1, output_layer_size))\n","\n","def feedforward(X):\n","    hidden_input = np.dot(X, weights_input_hidden) + bias_hidden\n","    hidden_output = tanh(hidden_input)\n","    output_input = np.dot(hidden_output, weights_hidden_output) + bias_output\n","    output = sigmoid(output_input)\n","    return output\n","\n","output = feedforward(X)\n","\n","print(\"Output probabilities for each input:\")\n","print(output)\n","predictions = (output > 0.5).astype(int)\n","print(\"\\nPredictions: \", predictions.flatten())\n"]},{"cell_type":"markdown","source":[],"metadata":{"id":"dN1CaY30rGFo"}},{"cell_type":"code","source":["import numpy as np\n","\n","def relu(x):\n","    return np.maximum(0, x)\n","\n","def relu_derivative(x):\n","    return (x > 0).astype(float)\n","\n","def softmax(x):\n","    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n","    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n","\n","def mse_loss(y_true, y_pred):\n","    return np.mean(np.square(y_true - y_pred))\n","\n","# Use 1D array for binary output\n","y = np.array([0, 1, 1, 0])  # 1D array\n","\n","input_layer_size = 2\n","hidden_layer_size = 2\n","output_layer_size = 2\n","np.random.seed(42)\n","\n","weights_input_hidden = np.random.randn(input_layer_size, hidden_layer_size)\n","bias_hidden = np.zeros((1, hidden_layer_size))\n","\n","weights_hidden_output = np.random.randn(hidden_layer_size, output_layer_size)\n","bias_output = np.zeros((1, output_layer_size))\n","\n","def feedforward(X):\n","    hidden_input = np.dot(X, weights_input_hidden) + bias_hidden\n","    hidden_output = relu(hidden_input)\n","    output_input = np.dot(hidden_output, weights_hidden_output) + bias_output\n","    output = softmax(output_input)\n","    return hidden_output, output\n","\n","def backpropagation(X, y, learning_rate=0.01):\n","    global weights_input_hidden, bias_hidden, weights_hidden_output, bias_output\n","    hidden_output, output = feedforward(X)\n","\n","    # Convert y to one-hot encoding for output\n","    y_one_hot = np.eye(output_layer_size)[y]\n","\n","    # Calculate the output error using MSE\n","    output_error = output - y_one_hot\n","    output_delta = output_error  # No need to scale by 2/n for MSE here\n","\n","    hidden_error = output_delta.dot(weights_hidden_output.T) * relu_derivative(hidden_output)\n","    hidden_delta = hidden_error\n","\n","    weights_hidden_output -= hidden_output.T.dot(output_delta) * learning_rate\n","    bias_output -= np.sum(output_delta, axis=0, keepdims=True) * learning_rate\n","    weights_input_hidden -= X.T.dot(hidden_delta) * learning_rate\n","    bias_hidden -= np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate\n","\n","# Train your network\n","for _ in range(10000):\n","  backpropagation(X, y, learning_rate=0.001)\n","\n","# Get final output\n","hidden_output, final_output = feedforward(X)\n","\n","print(\"Output probabilities for each input:\")\n","print(final_output)\n","predictions = np.argmax(final_output, axis=1)\n","print(\"\\nPredictions:\", predictions)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hCwcTRCyrH2G","executionInfo":{"status":"ok","timestamp":1729591322575,"user_tz":-330,"elapsed":1453,"user":{"displayName":"Nithesshkumar Jeer Sridhar","userId":"12749079037239522902"}},"outputId":"02859883-c313-4ad6-e1d9-1a1f15ad3a28"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Output probabilities for each input:\n","[[0.60233936 0.39766064]\n"," [0.18835642 0.81164358]\n"," [0.22977988 0.77022012]\n"," [0.86709808 0.13290192]]\n","\n","Predictions: [0 1 1 0]\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"OVplGRIhrH7m"}}]}