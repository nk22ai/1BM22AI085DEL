# -*- coding: utf-8 -*-
"""(26/11/2024)DELLAB(85).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h4iSYXHXvbAHUACgKLWgms9R9jI9fbvW
"""

#RNN networks
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout
from tensorflow.keras.preprocessing.sequence import pad_sequences
# Load the IMDb dataset with 10000 most frequent words
max_features = 10000  # Use top 10000 most frequent words
maxlen = 500  # Maximum length of each review

# Load the training and test data
(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)

# Pad the sequences to ensure they are of equal length
X_train = pad_sequences(X_train, maxlen=maxlen)
X_test = pad_sequences(X_test, maxlen=maxlen)

print(f"Training data shape: {X_train.shape}")
print(f"Test data shape: {X_test.shape}")
model = Sequential()

# Embedding layer to convert word indices into dense vectors
model.add(Embedding(input_dim=max_features, output_dim=128, input_length=maxlen))

# RNN layer (SimpleRNN) for sequence processing
model.add(SimpleRNN(128, activation='tanh'))

# Dropout to avoid overfitting
model.add(Dropout(0.2))

# Dense layer for classification (binary output)
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Display model summary
model.summary()
  # Train the model with the training data
history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))
# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(X_test, y_test)

print(f"Test Loss: {test_loss}")
print(f"Test Accuracy: {test_accuracy}")
import matplotlib.pyplot as plt

# Plot training and validation accuracy
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Test Accuracy')
plt.title('Training and Test Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plot training and validation loss
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Test Loss')
plt.title('Training and Test Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
# Load the IMDb dataset, with top 10000 most frequent words
max_features = 10000  # Number of unique words to consider
maxlen = 500  # Maximum sequence length

# Load the IMDb dataset
(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)

# Check the first sample in the training data
print("Sample Review (encoded):", X_train[0])
print("Sentiment (0 = Negative, 1 = Positive):", y_train[0])
# Example of raw text data for encoding
texts = ["I love this movie!", "This was an awful movie!"]

# Initialize a Tokenizer with max_features (word index)
tokenizer = Tokenizer(num_words=max_features)
tokenizer.fit_on_texts(texts)

# Convert text to sequences (lists of integers)
sequences = tokenizer.texts_to_sequences(texts)

# Pad sequences to ensure uniform input length
padded_sequences = pad_sequences(sequences, maxlen=maxlen)
print("Padded Sequences:", padded_sequences)
# Pad the training and test data to ensure all sequences are of the same length
X_train = pad_sequences(X_train, maxlen=maxlen)
X_test = pad_sequences(X_test, maxlen=maxlen)

# Check the shape of the padded sequences
print("Padded Training Data Shape:", X_train.shape)
print("Padded Test Data Shape:", X_test.shape)
# Initialize the model
model = Sequential()

# Add the embedding layer (converts word indices to dense vectors)
model.add(Embedding(input_dim=max_features, output_dim=128, input_length=maxlen))

# Add an RNN layer for sequence processing
model.add(SimpleRNN(128, activation='tanh'))

# Add Dropout for regularization
model.add(Dropout(0.2))

# Add a Dense layer for output, using sigmoid activation for binary classification
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Show the model summary
model.summary()
# Train the model using the training data
history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))
# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(X_test, y_test)

print(f"Test Loss: {test_loss}")
print(f"Test Accuracy: {test_accuracy}")
import matplotlib.pyplot as plt

# Plot the training and validation accuracy
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Test Accuracy')
plt.title('Training and Test Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plot the training and validation loss
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Test Loss')
plt.title('Training and Test Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# Example of a new review
new_review = ["This movie is amazing!"]

# Encode and pad the new review
new_sequence = tokenizer.texts_to_sequences(new_review)
new_padded_sequence = pad_sequences(new_sequence, maxlen=maxlen)

# Predict sentiment (0 for negative, 1 for positive)
prediction = model.predict(new_padded_sequence)

print(f"Predicted Sentiment: {'Positive' if prediction > 0.5 else 'Negative'}")